{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ShuffleNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ov7EmHf4i2xO",
        "colab_type": "text"
      },
      "source": [
        "model.cifar.vgg-cfiar.py\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHMv_34RP2nA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBiayO0oitI5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class ShuffleBlock(nn.Module):\n",
        "    def __init__(self, groups):\n",
        "        super(ShuffleBlock, self).__init__()\n",
        "        self.groups = groups\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''Channel shuffle: [N,C,H,W] -> [N,g,C/g,H,W] -> [N,C/g,g,H,w] -> [N,C,H,W]'''\n",
        "        N,C,H,W = x.size()\n",
        "        g = self.groups\n",
        "        return x.view(N,g,C//g,H,W).permute(0,2,1,3,4).reshape(N,C,H,W)\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, stride, groups):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.stride = stride\n",
        "\n",
        "        mid_planes = int(out_planes/4)\n",
        "        g = 1 if in_planes==24 else groups\n",
        "        self.conv1 = nn.Conv2d(in_planes, mid_planes, kernel_size=1, groups=g, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(mid_planes)\n",
        "        self.shuffle1 = ShuffleBlock(groups=g)\n",
        "        self.conv2 = nn.Conv2d(mid_planes, mid_planes, kernel_size=3, stride=stride, padding=1, groups=mid_planes, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(mid_planes)\n",
        "        self.conv3 = nn.Conv2d(mid_planes, out_planes, kernel_size=1, groups=groups, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride == 2:\n",
        "            self.shortcut = nn.Sequential(nn.AvgPool2d(3, stride=2, padding=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.shuffle1(out)\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        res = self.shortcut(x)\n",
        "        out = F.relu(torch.cat([out,res], 1)) if self.stride==2 else F.relu(out+res)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ShuffleNet(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super(ShuffleNet, self).__init__()\n",
        "        out_planes = cfg['out_planes']\n",
        "        num_blocks = cfg['num_blocks']\n",
        "        groups = cfg['groups']\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 24, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(24)\n",
        "        self.in_planes = 24\n",
        "        self.layer1 = self._make_layer(out_planes[0], num_blocks[0], groups)\n",
        "        self.layer2 = self._make_layer(out_planes[1], num_blocks[1], groups)\n",
        "        self.layer3 = self._make_layer(out_planes[2], num_blocks[2], groups)\n",
        "        self.linear = nn.Linear(out_planes[2], 100)\n",
        "\n",
        "    def _make_layer(self, out_planes, num_blocks, groups):\n",
        "        layers = []\n",
        "        for i in range(num_blocks):\n",
        "            stride = 2 if i == 0 else 1\n",
        "            cat_planes = self.in_planes if i == 0 else 0\n",
        "            layers.append(Bottleneck(self.in_planes, out_planes-cat_planes, stride=stride, groups=groups))\n",
        "            self.in_planes = out_planes\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ShuffleNetG2():\n",
        "    cfg = {\n",
        "        'out_planes': [200,400,800],\n",
        "        'num_blocks': [4,8,4],\n",
        "        'groups': 2\n",
        "    }\n",
        "    return ShuffleNet(cfg)\n",
        "\n",
        "def ShuffleNetG3():\n",
        "    cfg = {\n",
        "        'out_planes': [240,480,960],\n",
        "        'num_blocks': [4,8,4],\n",
        "        'groups': 3\n",
        "    }\n",
        "    return ShuffleNet(cfg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDTTk-VsjAPS",
        "colab_type": "text"
      },
      "source": [
        "config.py\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxauZ6_XimSu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import easydict\n",
        "\n",
        "\n",
        "def config():\n",
        "  \n",
        "    cfg = easydict.EasyDict({\n",
        "            \"arch\": \"shufflenet-cifar\",\n",
        "            \"dataset\": \"cifar100\",\n",
        "            \"batch_size\": 16,\n",
        "            \"epochs\": 100,\n",
        "            \"learning_rate\": 0.1,\n",
        "            \"weight_decay\": 0.00001,\n",
        "            \"momentum\": 0.9,\n",
        "            \"nesterov\": True,\n",
        "            \"print_freq\": 50,\n",
        "            \"ckpt\": \"/content/drive/My Drive/MLVC/Baseline/checkpoint/\",\n",
        "            \"results_dir\": \"./results/\",\n",
        "            \"resume\": False,\n",
        "            \"evaluate\": False,\n",
        "            \"cuda\": True,\n",
        "            \"gpuids\": [0],\n",
        "            \"colab\": True,    \n",
        "    })\n",
        "\n",
        "\n",
        "    cfg.gpuids = list(map(int, cfg.gpuids))\n",
        "\n",
        "    model = ShuffleNetG2()\n",
        "    if cfg.arch == \"shufflenet-cifar\":\n",
        "        model = ShuffleNetG2()\n",
        "    #elif cfg.arch == \"resnet-cifar\":\n",
        "    #    model = resnet.resnet20()\n",
        "    #elif cfg.arch == \"vgg-cifar-binary\":\n",
        "    #    model = vgg_bnn.vgg11()\n",
        "    #elif cfg.arch == \"resnet-cifar-dorefa\":\n",
        "    #    model = resnet_dorefanet.resnet20()\n",
        "\n",
        "    return cfg, model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uijg3T_jKpH",
        "colab_type": "text"
      },
      "source": [
        "utility.py\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRAxpnTajMvg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import time\n",
        "import shutil\n",
        "import pathlib\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "def load_model(model, ckpt_file, args):\n",
        "    if args.cuda:\n",
        "        checkpoint = torch.load(\n",
        "            ckpt_file, map_location=lambda storage, loc: storage.cuda(args.gpuids[0])\n",
        "        )\n",
        "        try:\n",
        "            model.load_state_dict(checkpoint[\"model\"])\n",
        "        except:  # noqa\n",
        "            model.module.load_state_dict(checkpoint[\"model\"])\n",
        "    else:\n",
        "        checkpoint = torch.load(ckpt_file, map_location=lambda storage, loc: storage)\n",
        "        try:\n",
        "            model.load_state_dict(checkpoint[\"model\"])\n",
        "        except:  # noqa\n",
        "            # create new OrderedDict that does not contain `module.`\n",
        "            new_state_dict = OrderedDict()\n",
        "            for k, v in checkpoint[\"model\"].items():\n",
        "                if k[:7] == \"module.\":\n",
        "                    name = k[7:]  # remove `module.`\n",
        "                else:\n",
        "                    name = k[:]\n",
        "                new_state_dict[name] = v\n",
        "\n",
        "            model.load_state_dict(new_state_dict)\n",
        "\n",
        "    return checkpoint\n",
        "\n",
        "\n",
        "def save_model(state, epoch, is_best, args):\n",
        "    dir_ckpt = pathlib.Path(\"checkpoint\")\n",
        "    dir_path = dir_ckpt / args.dataset\n",
        "    dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    model_file = dir_path / \"ckpt_epoch_{}.pth\".format(epoch)\n",
        "    torch.save(state, model_file)\n",
        "\n",
        "    if is_best:\n",
        "        shutil.copyfile(model_file, dir_path / \"ckpt_best.pth\")\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self, name, fmt=\":f\"):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = \"{name} {val\" + self.fmt + \"} ({avg\" + self.fmt + \"})\"\n",
        "        return fmtstr.format(**self.__dict__)\n",
        "\n",
        "\n",
        "class ProgressMeter(object):\n",
        "    def __init__(self, num_batches, *meters, prefix=\"\"):\n",
        "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
        "        self.meters = meters\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def print(self, batch):\n",
        "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
        "        entries += [str(meter) for meter in self.meters]\n",
        "        print(\"\\t\".join(entries))\n",
        "\n",
        "    def _get_batch_fmtstr(self, num_batches):\n",
        "        num_digits = len(str(num_batches // 1))\n",
        "        fmt = \"{:\" + str(num_digits) + \"d}\"\n",
        "        return \"[\" + fmt + \"/\" + fmt.format(num_batches) + \"]\"\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch, lr):\n",
        "    \"\"\"Sets the learning rate, decayed rate of 0.1 every epoch\"\"\"\n",
        "    if epoch >= 50:\n",
        "        lr = 0.01\n",
        "    if epoch >= 75:\n",
        "        lr = 0.001\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr\n",
        "\n",
        "    return lr\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res\n",
        "\n",
        "def print_reults(start_time, train_time, validate_time, start_epoch, epochs):\n",
        "    avg_train_time = train_time / (epochs - start_epoch)\n",
        "    avg_valid_time = validate_time / (epochs - start_epoch)\n",
        "    total_train_time = train_time + validate_time\n",
        "    print(\n",
        "        \"====> average training time per epoch: {:,}m {:.2f}s\".format(\n",
        "            int(avg_train_time // 60), avg_train_time % 60\n",
        "        )\n",
        "    )\n",
        "    print(\n",
        "        \"====> average validation time per epoch: {:,}m {:.2f}s\".format(\n",
        "            int(avg_valid_time // 60), avg_valid_time % 60\n",
        "        )\n",
        "    )\n",
        "    print(\n",
        "        \"====> training time: {}h {}m {:.2f}s\".format(\n",
        "            int(train_time // 3600), int((train_time % 3600) // 60), train_time % 60\n",
        "        )\n",
        "    )\n",
        "    print(\n",
        "        \"====> validation time: {}h {}m {:.2f}s\".format(\n",
        "            int(validate_time // 3600),\n",
        "            int((validate_time % 3600) // 60),\n",
        "            validate_time % 60,\n",
        "        )\n",
        "    )\n",
        "    print(\n",
        "        \"====> total training time: {}h {}m {:.2f}s\".format(\n",
        "            int(total_train_time // 3600),\n",
        "            int((total_train_time % 3600) // 60),\n",
        "            total_train_time % 60,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(\n",
        "        \"====> total time: {}h {}m {:.2f}s\".format(\n",
        "            int(elapsed_time // 3600), int((elapsed_time % 3600) // 60), elapsed_time % 60\n",
        "        )\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NYZJ-dXjZLX",
        "colab_type": "text"
      },
      "source": [
        "data_loader.py\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHxY6xojjcH3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "\n",
        "\n",
        "def dataloader(dataset, batch_size):\n",
        "    train_dataset, val_dataset = load_cifar100()\n",
        "\n",
        "    if dataset == \"CIFAR100\":\n",
        "        train_dataset, val_dataset = load_cifar100()\n",
        "\n",
        "    # Data loader\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        dataset=train_dataset, batch_size=batch_size, shuffle=True\n",
        "    )\n",
        "\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        dataset=val_dataset, batch_size=batch_size, shuffle=False\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "def load_cifar10():\n",
        "    # CIFAR-10 dataset\n",
        "    normalize = transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "\n",
        "    train_dataset = datasets.CIFAR10(\n",
        "        root=\"../../data/\",\n",
        "        train=True,\n",
        "        transform=transforms.Compose(\n",
        "            [\n",
        "                transforms.Pad(4),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomCrop(32),\n",
        "                transforms.ToTensor(),\n",
        "                normalize,\n",
        "            ]\n",
        "        ),\n",
        "        download=True,\n",
        "    )\n",
        "\n",
        "    val_dataset = datasets.CIFAR10(\n",
        "        root=\"../../data/\",\n",
        "        train=False,\n",
        "        transform=transforms.Compose([transforms.ToTensor(), normalize]),\n",
        "    )\n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "def load_cifar100():\n",
        "    # CIFAR-100 dataset\n",
        "    normalize = transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "    train_dataset = datasets.CIFAR100(\n",
        "        root=\"../../data/\",\n",
        "        train=True,\n",
        "        transform=transforms.Compose(\n",
        "            [\n",
        "                transforms.Pad(4),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomCrop(32),\n",
        "                transforms.ToTensor(),\n",
        "                normalize,\n",
        "            ]\n",
        "        ),\n",
        "        download=True,\n",
        "    )\n",
        "\n",
        "    val_dataset = datasets.CIFAR100(\n",
        "        root=\"../../data/\",\n",
        "        train=False,\n",
        "        transform=transforms.Compose([transforms.ToTensor(), normalize]),\n",
        "    )\n",
        "    return train_dataset, val_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bY8Jl99qjgEL",
        "colab_type": "text"
      },
      "source": [
        "main.py\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4lPApbXhvVw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import time\n",
        "import pathlib\n",
        "from os.path import isfile\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def main():\n",
        "    global args, start_epoch, best_acc1\n",
        "    args, model = config()\n",
        "\n",
        "    print(\"Model: {}\".format(args.arch))\n",
        "\n",
        "    if args.cuda and not torch.cuda.is_available():\n",
        "        raise Exception(\"No GPU found, please run without --cuda\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(\n",
        "        model.parameters(),\n",
        "        lr=args.learning_rate,\n",
        "        weight_decay=args.weight_decay,\n",
        "        momentum=args.momentum,\n",
        "        nesterov=args.nesterov,\n",
        "    )\n",
        "\n",
        "    best_acc1 = 0\n",
        "    start_epoch = 0\n",
        "\n",
        "    if args.cuda:\n",
        "        torch.cuda.set_device(args.gpuids[0])\n",
        "        with torch.cuda.device(args.gpuids[0]):\n",
        "            model = model.cuda()\n",
        "            criterion = criterion.cuda()\n",
        "        model = nn.DataParallel(\n",
        "            model, device_ids=args.gpuids, output_device=args.gpuids[0]\n",
        "        )\n",
        "        cudnn.benchmark = True\n",
        "\n",
        "    # checkpoint file\n",
        "    ckpt_dir = pathlib.Path(args.ckpt)\n",
        "    ckpt_file = ckpt_dir / args.dataset / args.ckpt\n",
        "\n",
        "    # for resuming training\n",
        "    if args.resume:\n",
        "        retrain(ckpt_file, model, optimizer)\n",
        "\n",
        "    # Data loading\n",
        "    print(\"\\n==> Load data..\")\n",
        "    train_loader, val_loader = dataloader(args.dataset, args.batch_size)\n",
        "\n",
        "    # initiailizae\n",
        "    train_time, validate_time = 0.0, 0.0\n",
        "    avgloss_train = 0.0\n",
        "    acc1_train, acc5_train, acc1_valid, acc5_valid = 0.0, 0.0, 0.0, 0.0\n",
        "    is_best = False\n",
        "\n",
        "    # result lists\n",
        "    result_epoch, result_lr, result_train_avgtime, result_train_avgloss = [], [], [], []\n",
        "    result_train_avgtop1acc, result_train_avgtop5acc = [], []\n",
        "    result_val_avgtime, result_val_avgtop1acc, result_val_avgtop5acc = [], [], []\n",
        "\n",
        "    # train...\n",
        "    lr = args.learning_rate\n",
        "    curr_lr = lr\n",
        "    for epoch in range(start_epoch, args.epochs):\n",
        "        curr_lr = adjust_learning_rate(optimizer, epoch, lr)\n",
        "        print(\"\\n==> Epoch: {}, lr = {}\".format(epoch, optimizer.param_groups[0][\"lr\"]))\n",
        "\n",
        "        # train for one epoch\n",
        "        train_time, acc1_train, acc5_train, avgloss_train = train_epoch(\n",
        "            train_time,\n",
        "            acc1_train,\n",
        "            acc5_train,\n",
        "            avgloss_train,\n",
        "            train_loader,\n",
        "            epoch,\n",
        "            model,\n",
        "            criterion,\n",
        "            optimizer,\n",
        "        )\n",
        "\n",
        "        # evaluate on validation set\n",
        "        validate_time, acc1_valid, acc5_valid = validation_epoch(\n",
        "            validate_time, acc1_valid, acc5_valid, val_loader, model, criterion\n",
        "        )\n",
        "\n",
        "        # remember best Acc@1 and save checkpoint\n",
        "        is_best = save_model_data(\n",
        "            is_best, best_acc1, acc1_valid, epoch, model, optimizer, args\n",
        "        )\n",
        "\n",
        "\n",
        "        result_epoch.append(epoch)\n",
        "        result_lr.append(curr_lr)\n",
        "        result_train_avgtime.append(train_time)\n",
        "        result_train_avgloss.append(avgloss_train)\n",
        "        result_train_avgtop1acc.append(acc1_train.item())\n",
        "        result_train_avgtop5acc.append(acc5_train.item())\n",
        "        result_val_avgtop1acc.append(acc1_valid.item())\n",
        "        result_val_avgtop5acc.append(acc5_valid.item())\n",
        "\n",
        "        df = pd.DataFrame({\n",
        "            'Epoch': result_epoch,\n",
        "            'Learning rate': result_lr,\n",
        "            'Training avg loss': result_train_avgloss,\n",
        "            'Training avg top1 acc': result_train_avgtop1acc,\n",
        "            'Training avg top5 acc': result_train_avgtop5acc,\n",
        "            'Test avg top1 acc': result_val_avgtop1acc,\n",
        "            'Test avg top5 acc': result_val_avgtop5acc,\n",
        "        })\n",
        "\n",
        "        if args.colab:\n",
        "            df.to_csv('/content/drive/My Drive/MLVC/Baseline/results/{}_result.csv'.format(args.arch))\n",
        "        else:\n",
        "            df.to_csv('./results/{}_result.csv'.format(args.arch))\n",
        "\n",
        "\n",
        "    print_results(train_time, validate_time)\n",
        "\n",
        "\n",
        "def retrain(ckpt_file, model, optimizer):\n",
        "    if isfile(ckpt_file):\n",
        "        print(\"\\n==> Loading Checkpoint '{}'\".format(args.ckpt))\n",
        "        checkpoint = load_model(model, ckpt_file, args)\n",
        "\n",
        "        start_epoch = checkpoint[\"epoch\"]\n",
        "        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "\n",
        "        print(\"==> Loaded Checkpoint '{}' (epoch {})\".format(args.ckpt, start_epoch))\n",
        "    else:\n",
        "        print(\"==> no checkpoint found '{}'\".format(args.ckpt))\n",
        "        return\n",
        "\n",
        "\n",
        "def train_epoch(\n",
        "    train_time, acc1_train, acc5_train, avgloss_train, train_loader, epoch, model, criterion, optimizer\n",
        "):\n",
        "    print(\"===> [ Training ]\")\n",
        "    start_time = time.time()\n",
        "    acc1_train, acc5_train, avgloss_train = train(\n",
        "        train_loader, epoch=epoch, model=model, criterion=criterion, optimizer=optimizer\n",
        "    )\n",
        "    elapsed_time = time.time() - start_time\n",
        "    train_time += elapsed_time\n",
        "    print(\"====> {:.2f} seconds to train this epoch\\n\".format(elapsed_time))\n",
        "\n",
        "    return train_time, acc1_train, acc5_train, avgloss_train\n",
        "\n",
        "\n",
        "def validation_epoch(\n",
        "    validate_time, acc1_valid, acc5_valid, val_loader, model, criterion\n",
        "):\n",
        "    print(\"===> [ Validation ]\")\n",
        "    start_time = time.time()\n",
        "    acc1_valid, acc5_valid, avgloss_valid = validate(val_loader, model, criterion)\n",
        "    elapsed_time = time.time() - start_time\n",
        "    validate_time += elapsed_time\n",
        "    print(\"====> {:.2f} seconds to validate this epoch\\n\".format(elapsed_time))\n",
        "\n",
        "    return validate_time, acc1_valid, acc5_valid\n",
        "\n",
        "\n",
        "def save_model_data(is_best, best_acc1, acc1_valid, epoch, model, optimizer, args):\n",
        "    is_best = acc1_valid > best_acc1\n",
        "    best_acc1 = max(acc1_valid, best_acc1)\n",
        "    state = {\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"model\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "    }\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "        save_model(state, epoch, is_best, args)\n",
        "    return is_best\n",
        "\n",
        "\n",
        "def train(train_loader, **kwargs):\n",
        "    epoch = kwargs.get(\"epoch\")\n",
        "    model = kwargs.get(\"model\")\n",
        "    criterion = kwargs.get(\"criterion\")\n",
        "    optimizer = kwargs.get(\"optimizer\")\n",
        "\n",
        "    batch_time = AverageMeter(\"Time\", \":6.3f\")\n",
        "    data_time = AverageMeter(\"Data\", \":6.3f\")\n",
        "    losses = AverageMeter(\"Loss\", \":.4e\")\n",
        "    top1 = AverageMeter(\"Acc@1\", \":6.2f\")\n",
        "    top5 = AverageMeter(\"Acc@5\", \":6.2f\")\n",
        "    progress = ProgressMeter(\n",
        "        len(train_loader),\n",
        "        batch_time,\n",
        "        data_time,\n",
        "        losses,\n",
        "        top1,\n",
        "        top5,\n",
        "        prefix=\"Epoch: [{}]\".format(epoch),\n",
        "    )\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "    running_loss = 0.0\n",
        "    for i, (input, target) in enumerate(train_loader):\n",
        "        # measure data loading time\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        if args.cuda:\n",
        "            input = input.cuda(non_blocking=True)\n",
        "            target = target.cuda(non_blocking=True)\n",
        "\n",
        "        # compute output\n",
        "        output = model(input)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "        losses.update(loss.item(), input.size(0))\n",
        "        top1.update(acc1[0], input.size(0))\n",
        "        top5.update(acc5[0], input.size(0))\n",
        "\n",
        "        # compute gradient and do SGD step.\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % args.print_freq == 0:\n",
        "            progress.print(i)\n",
        "\n",
        "        end = time.time()\n",
        "\n",
        "    print(\n",
        "        \"====> Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}\".format(top1=top1, top5=top5)\n",
        "    )\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    print(\"====> Epoch loss {:.3f}\".format(epoch_loss))\n",
        "\n",
        "    return top1.avg, top5.avg, epoch_loss\n",
        "\n",
        "\n",
        "def validate(val_loader, model, criterion):\n",
        "    batch_time = AverageMeter(\"Time\", \":6.3f\")\n",
        "    losses = AverageMeter(\"Loss\", \":.4e\")\n",
        "    top1 = AverageMeter(\"Acc@1\", \":6.2f\")\n",
        "    top5 = AverageMeter(\"Acc@5\", \":6.2f\")\n",
        "    progress = ProgressMeter(\n",
        "        len(val_loader), batch_time, losses, top1, top5, prefix=\"Test: \"\n",
        "    )\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        end = time.time()\n",
        "        for i, (input, target) in enumerate(val_loader):\n",
        "\n",
        "            if args.cuda:\n",
        "                input = input.cuda(non_blocking=True)\n",
        "                target = target.cuda(non_blocking=True)\n",
        "\n",
        "            # compute output\n",
        "            output = model(input)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "            losses.update(loss.item(), input.size(0))\n",
        "            top1.update(acc1[0], input.size(0))\n",
        "            top5.update(acc5[0], input.size(0))\n",
        "\n",
        "            # measure elapsed time\n",
        "            batch_time.update(time.time() - end)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if i % args.print_freq == 0:\n",
        "                progress.print(i)\n",
        "\n",
        "            end = time.time()\n",
        "\n",
        "        print(\n",
        "            \"====> Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}\".format(\n",
        "                top1=top1, top5=top5\n",
        "            )\n",
        "        )\n",
        "        total_loss = total_loss / len(val_loader)\n",
        "\n",
        "    return top1.avg, top5.avg, loss.item()\n",
        "\n",
        "\n",
        "def print_results(train_time, validate_time):\n",
        "\n",
        "    avg_train_time = train_time / (args.epochs - start_epoch)\n",
        "    avg_valid_time = validate_time / (args.epochs - start_epoch)\n",
        "    total_train_time = train_time + validate_time\n",
        "    print(\n",
        "        \"====> average training time per epoch: {:,}m {:.2f}s\".format(\n",
        "            int(avg_train_time // 60), avg_train_time % 60\n",
        "        )\n",
        "    )\n",
        "    print(\n",
        "        \"====> average validation time per epoch: {:,}m {:.2f}s\".format(\n",
        "            int(avg_valid_time // 60), avg_valid_time % 60\n",
        "        )\n",
        "    )\n",
        "    print(\n",
        "        \"====> training time: {}h {}m {:.2f}s\".format(\n",
        "            int(train_time // 3600), int((train_time % 3600) // 60), train_time % 60\n",
        "        )\n",
        "    )\n",
        "    print(\n",
        "        \"====> validation time: {}h {}m {:.2f}s\".format(\n",
        "            int(validate_time // 3600),\n",
        "            int((validate_time % 3600) // 60),\n",
        "            validate_time % 60,\n",
        "        )\n",
        "    )\n",
        "    print(\n",
        "        \"====> total training time: {}h {}m {:.2f}s\".format(\n",
        "            int(total_train_time // 3600),\n",
        "            int((total_train_time % 3600) // 60),\n",
        "            total_train_time % 60,\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    start_time = time.time()\n",
        "    main()\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(\n",
        "        \"====> total time: {}h {}m {:.2f}s\".format(\n",
        "            int(elapsed_time // 3600),\n",
        "            int((elapsed_time % 3600) // 60),\n",
        "            elapsed_time % 60,\n",
        "        )\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}