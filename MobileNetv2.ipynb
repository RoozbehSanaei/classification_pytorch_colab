{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MobileNetv2",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM0Rr7f1bZQQSw98rJHLQ4O"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1DnfRi4SzgM",
        "colab_type": "code",
        "outputId": "3e83e8f4-24e1-4b54-ac37-7b9092298395",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# An implementation of https://arxiv.org/pdf/1512.03385.pdf                    #\n",
        "# See section 4.2 for the model architecture on CIFAR-10                       #\n",
        "# Some part of the code was referenced from below                              #\n",
        "# https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
        "# https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py\n",
        "# https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/deep_residual_network/main.py\n",
        "# ---------------------------------------------------------------------------- #\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters\n",
        "num_epochs = 80\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Image preprocessing modules\n",
        "transform = transforms.Compose([\n",
        "    transforms.Pad(4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32),\n",
        "    transforms.ToTensor()])\n",
        "\n",
        "# CIFAR-10 dataset\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
        "                                             train=True, \n",
        "                                             transform=transform,\n",
        "                                             download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
        "                                            train=False, \n",
        "                                            transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=100, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=100, \n",
        "                                          shuffle=False)\n",
        "\n",
        "\n",
        "__all__ = ['MobileNetV2', 'mobilenet_v2']\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'mobilenet_v2': 'https://download.pytorch.org/models/mobilenet_v2-b0353104.pth',\n",
        "}\n",
        "\n",
        "\n",
        "def _make_divisible(v, divisor, min_value=None):\n",
        "    \"\"\"\n",
        "    This function is taken from the original tf repo.\n",
        "    It ensures that all layers have a channel number that is divisible by 8\n",
        "    It can be seen here:\n",
        "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
        "    :param v:\n",
        "    :param divisor:\n",
        "    :param min_value:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        new_v += divisor\n",
        "    return new_v\n",
        "\n",
        "\n",
        "class ConvBNReLU(nn.Sequential):\n",
        "    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, groups=1):\n",
        "        padding = (kernel_size - 1) // 2\n",
        "        super(ConvBNReLU, self).__init__(\n",
        "            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups, bias=False),\n",
        "            nn.BatchNorm2d(out_planes),\n",
        "            nn.ReLU6(inplace=True)\n",
        "        )\n",
        "\n",
        "\n",
        "class InvertedResidual(nn.Module):\n",
        "    def __init__(self, inp, oup, stride, expand_ratio):\n",
        "        super(InvertedResidual, self).__init__()\n",
        "        self.stride = stride\n",
        "        assert stride in [1, 2]\n",
        "\n",
        "        hidden_dim = int(round(inp * expand_ratio))\n",
        "        self.use_res_connect = self.stride == 1 and inp == oup\n",
        "\n",
        "        layers = []\n",
        "        if expand_ratio != 1:\n",
        "            # pw\n",
        "            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1))\n",
        "        layers.extend([\n",
        "            # dw\n",
        "            ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim),\n",
        "            # pw-linear\n",
        "            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(oup),\n",
        "        ])\n",
        "        self.conv = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_res_connect:\n",
        "            return x + self.conv(x)\n",
        "        else:\n",
        "            return self.conv(x)\n",
        "\n",
        "\n",
        "class MobileNetV2(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_classes=1000,\n",
        "                 width_mult=1.0,\n",
        "                 inverted_residual_setting=None,\n",
        "                 round_nearest=8,\n",
        "                 block=None):\n",
        "        \"\"\"\n",
        "        MobileNet V2 main class\n",
        "        Args:\n",
        "            num_classes (int): Number of classes\n",
        "            width_mult (float): Width multiplier - adjusts number of channels in each layer by this amount\n",
        "            inverted_residual_setting: Network structure\n",
        "            round_nearest (int): Round the number of channels in each layer to be a multiple of this number\n",
        "            Set to 1 to turn off rounding\n",
        "            block: Module specifying inverted residual building block for mobilenet\n",
        "        \"\"\"\n",
        "        super(MobileNetV2, self).__init__()\n",
        "\n",
        "        if block is None:\n",
        "            block = InvertedResidual\n",
        "        input_channel = 32\n",
        "        last_channel = 1280\n",
        "\n",
        "        if inverted_residual_setting is None:\n",
        "            inverted_residual_setting = [\n",
        "                # t, c, n, s\n",
        "                [1, 16, 1, 1],\n",
        "                [6, 24, 2, 2],\n",
        "                [6, 32, 3, 2],\n",
        "                [6, 64, 4, 2],\n",
        "                [6, 96, 3, 1],\n",
        "                [6, 160, 3, 2],\n",
        "                [6, 320, 1, 1],\n",
        "            ]\n",
        "\n",
        "        # only check the first element, assuming user knows t,c,n,s are required\n",
        "        if len(inverted_residual_setting) == 0 or len(inverted_residual_setting[0]) != 4:\n",
        "            raise ValueError(\"inverted_residual_setting should be non-empty \"\n",
        "                             \"or a 4-element list, got {}\".format(inverted_residual_setting))\n",
        "\n",
        "        # building first layer\n",
        "        input_channel = _make_divisible(input_channel * width_mult, round_nearest)\n",
        "        self.last_channel = _make_divisible(last_channel * max(1.0, width_mult), round_nearest)\n",
        "        features = [ConvBNReLU(3, input_channel, stride=2)]\n",
        "        # building inverted residual blocks\n",
        "        for t, c, n, s in inverted_residual_setting:\n",
        "            output_channel = _make_divisible(c * width_mult, round_nearest)\n",
        "            for i in range(n):\n",
        "                stride = s if i == 0 else 1\n",
        "                features.append(block(input_channel, output_channel, stride, expand_ratio=t))\n",
        "                input_channel = output_channel\n",
        "        # building last several layers\n",
        "        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1))\n",
        "        # make it nn.Sequential\n",
        "        self.features = nn.Sequential(*features)\n",
        "\n",
        "        # building classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(self.last_channel, num_classes),\n",
        "        )\n",
        "\n",
        "        # weight initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.ones_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def _forward_impl(self, x):\n",
        "        # This exists since TorchScript doesn't support inheritance, so the superclass method\n",
        "        # (this one) needs to have a name other than `forward` that can be accessed in a subclass\n",
        "        x = self.features(x)\n",
        "        x = x.mean([2, 3])\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._forward_impl(x)\n",
        "\n",
        "\n",
        "def mobilenet_v2(pretrained=False, progress=True, **kwargs):\n",
        "    \"\"\"\n",
        "    Constructs a MobileNetV2 architecture from\n",
        "    `\"MobileNetV2: Inverted Residuals and Linear Bottlenecks\" <https://arxiv.org/abs/1801.04381>`_.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    model = MobileNetV2(**kwargs)\n",
        "    if pretrained:\n",
        "        state_dict = load_state_dict_from_url(model_urls['mobilenet_v2'],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "model = mobilenet_v2().to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# For updating learning rate\n",
        "def update_lr(optimizer, lr):    \n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "curr_lr = learning_rate\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print (\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\"\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "    # Decay learning rate\n",
        "    if (epoch+1) % 20 == 0:\n",
        "        curr_lr /= 3\n",
        "        update_lr(optimizer, curr_lr)\n",
        "\n",
        "# Test the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n",
        "\n",
        "# Save the model checkpoint\n",
        "torch.save(model.state_dict(), 'resnet.ckpt')\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Epoch [1/80], Step [100/500] Loss: 2.2268\n",
            "Epoch [1/80], Step [200/500] Loss: 1.8905\n",
            "Epoch [1/80], Step [300/500] Loss: 1.9160\n",
            "Epoch [1/80], Step [400/500] Loss: 1.8599\n",
            "Epoch [1/80], Step [500/500] Loss: 1.7288\n",
            "Epoch [2/80], Step [100/500] Loss: 1.7716\n",
            "Epoch [2/80], Step [200/500] Loss: 1.5087\n",
            "Epoch [2/80], Step [300/500] Loss: 1.6819\n",
            "Epoch [2/80], Step [400/500] Loss: 1.5505\n",
            "Epoch [2/80], Step [500/500] Loss: 1.7962\n",
            "Epoch [3/80], Step [100/500] Loss: 1.4340\n",
            "Epoch [3/80], Step [200/500] Loss: 1.6596\n",
            "Epoch [3/80], Step [300/500] Loss: 1.5860\n",
            "Epoch [3/80], Step [400/500] Loss: 1.5389\n",
            "Epoch [3/80], Step [500/500] Loss: 1.4344\n",
            "Epoch [4/80], Step [100/500] Loss: 1.4326\n",
            "Epoch [4/80], Step [200/500] Loss: 1.3353\n",
            "Epoch [4/80], Step [300/500] Loss: 1.4658\n",
            "Epoch [4/80], Step [400/500] Loss: 1.2096\n",
            "Epoch [4/80], Step [500/500] Loss: 1.2354\n",
            "Epoch [5/80], Step [100/500] Loss: 1.1284\n",
            "Epoch [5/80], Step [200/500] Loss: 1.2797\n",
            "Epoch [5/80], Step [300/500] Loss: 1.4768\n",
            "Epoch [5/80], Step [400/500] Loss: 1.2102\n",
            "Epoch [5/80], Step [500/500] Loss: 1.1210\n",
            "Epoch [6/80], Step [100/500] Loss: 1.3097\n",
            "Epoch [6/80], Step [200/500] Loss: 1.2763\n",
            "Epoch [6/80], Step [300/500] Loss: 1.2359\n",
            "Epoch [6/80], Step [400/500] Loss: 1.2151\n",
            "Epoch [6/80], Step [500/500] Loss: 1.1054\n",
            "Epoch [7/80], Step [100/500] Loss: 1.0827\n",
            "Epoch [7/80], Step [200/500] Loss: 1.1607\n",
            "Epoch [7/80], Step [300/500] Loss: 0.9568\n",
            "Epoch [7/80], Step [400/500] Loss: 0.9611\n",
            "Epoch [7/80], Step [500/500] Loss: 1.2127\n",
            "Epoch [8/80], Step [100/500] Loss: 1.1553\n",
            "Epoch [8/80], Step [200/500] Loss: 1.2772\n",
            "Epoch [8/80], Step [300/500] Loss: 1.0201\n",
            "Epoch [8/80], Step [400/500] Loss: 1.2139\n",
            "Epoch [8/80], Step [500/500] Loss: 1.1495\n",
            "Epoch [9/80], Step [100/500] Loss: 1.1305\n",
            "Epoch [9/80], Step [200/500] Loss: 1.0368\n",
            "Epoch [9/80], Step [300/500] Loss: 0.8892\n",
            "Epoch [9/80], Step [400/500] Loss: 1.0753\n",
            "Epoch [9/80], Step [500/500] Loss: 0.8723\n",
            "Epoch [10/80], Step [100/500] Loss: 0.9589\n",
            "Epoch [10/80], Step [200/500] Loss: 1.0341\n",
            "Epoch [10/80], Step [300/500] Loss: 0.8769\n",
            "Epoch [10/80], Step [400/500] Loss: 0.9892\n",
            "Epoch [10/80], Step [500/500] Loss: 1.1145\n",
            "Epoch [11/80], Step [100/500] Loss: 0.9746\n",
            "Epoch [11/80], Step [200/500] Loss: 0.9521\n",
            "Epoch [11/80], Step [300/500] Loss: 0.8267\n",
            "Epoch [11/80], Step [400/500] Loss: 0.8373\n",
            "Epoch [11/80], Step [500/500] Loss: 0.6746\n",
            "Epoch [12/80], Step [100/500] Loss: 1.1243\n",
            "Epoch [12/80], Step [200/500] Loss: 0.9285\n",
            "Epoch [12/80], Step [300/500] Loss: 0.7284\n",
            "Epoch [12/80], Step [400/500] Loss: 0.9782\n",
            "Epoch [12/80], Step [500/500] Loss: 0.9605\n",
            "Epoch [13/80], Step [100/500] Loss: 0.8927\n",
            "Epoch [13/80], Step [200/500] Loss: 0.7820\n",
            "Epoch [13/80], Step [300/500] Loss: 0.7683\n",
            "Epoch [13/80], Step [400/500] Loss: 0.7356\n",
            "Epoch [13/80], Step [500/500] Loss: 0.6176\n",
            "Epoch [14/80], Step [100/500] Loss: 1.0445\n",
            "Epoch [14/80], Step [200/500] Loss: 0.8840\n",
            "Epoch [14/80], Step [300/500] Loss: 0.9297\n",
            "Epoch [14/80], Step [400/500] Loss: 1.0577\n",
            "Epoch [14/80], Step [500/500] Loss: 0.7170\n",
            "Epoch [15/80], Step [100/500] Loss: 1.0663\n",
            "Epoch [15/80], Step [200/500] Loss: 0.6647\n",
            "Epoch [15/80], Step [300/500] Loss: 0.9049\n",
            "Epoch [15/80], Step [400/500] Loss: 0.8518\n",
            "Epoch [15/80], Step [500/500] Loss: 0.8626\n",
            "Epoch [16/80], Step [100/500] Loss: 0.7440\n",
            "Epoch [16/80], Step [200/500] Loss: 0.8078\n",
            "Epoch [16/80], Step [300/500] Loss: 0.8848\n",
            "Epoch [16/80], Step [400/500] Loss: 0.8097\n",
            "Epoch [16/80], Step [500/500] Loss: 0.8036\n",
            "Epoch [17/80], Step [100/500] Loss: 0.6513\n",
            "Epoch [17/80], Step [200/500] Loss: 0.8717\n",
            "Epoch [17/80], Step [300/500] Loss: 0.7027\n",
            "Epoch [17/80], Step [400/500] Loss: 0.8173\n",
            "Epoch [17/80], Step [500/500] Loss: 0.8166\n",
            "Epoch [18/80], Step [100/500] Loss: 0.8206\n",
            "Epoch [18/80], Step [200/500] Loss: 0.6555\n",
            "Epoch [18/80], Step [300/500] Loss: 0.9721\n",
            "Epoch [18/80], Step [400/500] Loss: 0.8317\n",
            "Epoch [18/80], Step [500/500] Loss: 0.7668\n",
            "Epoch [19/80], Step [100/500] Loss: 0.6201\n",
            "Epoch [19/80], Step [200/500] Loss: 0.8147\n",
            "Epoch [19/80], Step [300/500] Loss: 0.6714\n",
            "Epoch [19/80], Step [400/500] Loss: 0.7371\n",
            "Epoch [19/80], Step [500/500] Loss: 0.8249\n",
            "Epoch [20/80], Step [100/500] Loss: 0.5532\n",
            "Epoch [20/80], Step [200/500] Loss: 0.7364\n",
            "Epoch [20/80], Step [300/500] Loss: 0.5739\n",
            "Epoch [20/80], Step [400/500] Loss: 0.7006\n",
            "Epoch [20/80], Step [500/500] Loss: 0.6686\n",
            "Epoch [21/80], Step [100/500] Loss: 0.6785\n",
            "Epoch [21/80], Step [200/500] Loss: 0.7518\n",
            "Epoch [21/80], Step [300/500] Loss: 0.6777\n",
            "Epoch [21/80], Step [400/500] Loss: 0.5322\n",
            "Epoch [21/80], Step [500/500] Loss: 0.6733\n",
            "Epoch [22/80], Step [100/500] Loss: 0.5908\n",
            "Epoch [22/80], Step [200/500] Loss: 0.5183\n",
            "Epoch [22/80], Step [300/500] Loss: 0.5720\n",
            "Epoch [22/80], Step [400/500] Loss: 0.5574\n",
            "Epoch [22/80], Step [500/500] Loss: 0.6125\n",
            "Epoch [23/80], Step [100/500] Loss: 0.6750\n",
            "Epoch [23/80], Step [200/500] Loss: 0.5277\n",
            "Epoch [23/80], Step [300/500] Loss: 0.5001\n",
            "Epoch [23/80], Step [400/500] Loss: 0.5635\n",
            "Epoch [23/80], Step [500/500] Loss: 0.5558\n",
            "Epoch [24/80], Step [100/500] Loss: 0.7143\n",
            "Epoch [24/80], Step [200/500] Loss: 0.7245\n",
            "Epoch [24/80], Step [300/500] Loss: 0.6819\n",
            "Epoch [24/80], Step [400/500] Loss: 0.4877\n",
            "Epoch [24/80], Step [500/500] Loss: 0.7020\n",
            "Epoch [25/80], Step [100/500] Loss: 0.6604\n",
            "Epoch [25/80], Step [200/500] Loss: 0.5495\n",
            "Epoch [25/80], Step [300/500] Loss: 0.6456\n",
            "Epoch [25/80], Step [400/500] Loss: 0.5218\n",
            "Epoch [25/80], Step [500/500] Loss: 0.6760\n",
            "Epoch [26/80], Step [100/500] Loss: 0.5481\n",
            "Epoch [26/80], Step [200/500] Loss: 0.6917\n",
            "Epoch [26/80], Step [300/500] Loss: 0.6419\n",
            "Epoch [26/80], Step [400/500] Loss: 0.4487\n",
            "Epoch [26/80], Step [500/500] Loss: 0.6160\n",
            "Epoch [27/80], Step [100/500] Loss: 0.5082\n",
            "Epoch [27/80], Step [200/500] Loss: 0.7206\n",
            "Epoch [27/80], Step [300/500] Loss: 0.6091\n",
            "Epoch [27/80], Step [400/500] Loss: 0.5483\n",
            "Epoch [27/80], Step [500/500] Loss: 0.4760\n",
            "Epoch [28/80], Step [100/500] Loss: 0.6441\n",
            "Epoch [28/80], Step [200/500] Loss: 0.6756\n",
            "Epoch [28/80], Step [300/500] Loss: 0.5090\n",
            "Epoch [28/80], Step [400/500] Loss: 0.5017\n",
            "Epoch [28/80], Step [500/500] Loss: 0.4288\n",
            "Epoch [29/80], Step [100/500] Loss: 0.5647\n",
            "Epoch [29/80], Step [200/500] Loss: 0.4418\n",
            "Epoch [29/80], Step [300/500] Loss: 0.5599\n",
            "Epoch [29/80], Step [400/500] Loss: 0.5769\n",
            "Epoch [29/80], Step [500/500] Loss: 0.6876\n",
            "Epoch [30/80], Step [100/500] Loss: 0.4680\n",
            "Epoch [30/80], Step [200/500] Loss: 0.6565\n",
            "Epoch [30/80], Step [300/500] Loss: 0.6735\n",
            "Epoch [30/80], Step [400/500] Loss: 0.5745\n",
            "Epoch [30/80], Step [500/500] Loss: 0.7114\n",
            "Epoch [31/80], Step [100/500] Loss: 0.5743\n",
            "Epoch [31/80], Step [200/500] Loss: 0.5618\n",
            "Epoch [31/80], Step [300/500] Loss: 0.5421\n",
            "Epoch [31/80], Step [400/500] Loss: 0.4617\n",
            "Epoch [31/80], Step [500/500] Loss: 0.6192\n",
            "Epoch [32/80], Step [100/500] Loss: 0.5581\n",
            "Epoch [32/80], Step [200/500] Loss: 0.5538\n",
            "Epoch [32/80], Step [300/500] Loss: 0.7124\n",
            "Epoch [32/80], Step [400/500] Loss: 0.3831\n",
            "Epoch [32/80], Step [500/500] Loss: 0.5447\n",
            "Epoch [33/80], Step [100/500] Loss: 0.5578\n",
            "Epoch [33/80], Step [200/500] Loss: 0.4989\n",
            "Epoch [33/80], Step [300/500] Loss: 0.5442\n",
            "Epoch [33/80], Step [400/500] Loss: 0.5387\n",
            "Epoch [33/80], Step [500/500] Loss: 0.4924\n",
            "Epoch [34/80], Step [100/500] Loss: 0.5330\n",
            "Epoch [34/80], Step [200/500] Loss: 0.5070\n",
            "Epoch [34/80], Step [300/500] Loss: 0.3670\n",
            "Epoch [34/80], Step [400/500] Loss: 0.3845\n",
            "Epoch [34/80], Step [500/500] Loss: 0.3892\n",
            "Epoch [35/80], Step [100/500] Loss: 0.5941\n",
            "Epoch [35/80], Step [200/500] Loss: 0.5087\n",
            "Epoch [35/80], Step [300/500] Loss: 0.4984\n",
            "Epoch [35/80], Step [400/500] Loss: 0.4743\n",
            "Epoch [35/80], Step [500/500] Loss: 0.6655\n",
            "Epoch [36/80], Step [100/500] Loss: 0.3688\n",
            "Epoch [36/80], Step [200/500] Loss: 0.5666\n",
            "Epoch [36/80], Step [300/500] Loss: 0.5257\n",
            "Epoch [36/80], Step [400/500] Loss: 0.6679\n",
            "Epoch [36/80], Step [500/500] Loss: 0.6340\n",
            "Epoch [37/80], Step [100/500] Loss: 0.4356\n",
            "Epoch [37/80], Step [200/500] Loss: 0.3800\n",
            "Epoch [37/80], Step [300/500] Loss: 0.3704\n",
            "Epoch [37/80], Step [400/500] Loss: 0.5802\n",
            "Epoch [37/80], Step [500/500] Loss: 0.7691\n",
            "Epoch [38/80], Step [100/500] Loss: 0.3668\n",
            "Epoch [38/80], Step [200/500] Loss: 0.4309\n",
            "Epoch [38/80], Step [300/500] Loss: 0.6005\n",
            "Epoch [38/80], Step [400/500] Loss: 0.4108\n",
            "Epoch [38/80], Step [500/500] Loss: 0.4805\n",
            "Epoch [39/80], Step [100/500] Loss: 0.5160\n",
            "Epoch [39/80], Step [200/500] Loss: 0.5123\n",
            "Epoch [39/80], Step [300/500] Loss: 0.4697\n",
            "Epoch [39/80], Step [400/500] Loss: 0.4321\n",
            "Epoch [39/80], Step [500/500] Loss: 0.2464\n",
            "Epoch [40/80], Step [100/500] Loss: 0.5191\n",
            "Epoch [40/80], Step [200/500] Loss: 0.4454\n",
            "Epoch [40/80], Step [300/500] Loss: 0.5195\n",
            "Epoch [40/80], Step [400/500] Loss: 0.5510\n",
            "Epoch [40/80], Step [500/500] Loss: 0.6370\n",
            "Epoch [41/80], Step [100/500] Loss: 0.7008\n",
            "Epoch [41/80], Step [200/500] Loss: 0.3832\n",
            "Epoch [41/80], Step [300/500] Loss: 0.4433\n",
            "Epoch [41/80], Step [400/500] Loss: 0.4289\n",
            "Epoch [41/80], Step [500/500] Loss: 0.4712\n",
            "Epoch [42/80], Step [100/500] Loss: 0.4407\n",
            "Epoch [42/80], Step [200/500] Loss: 0.5109\n",
            "Epoch [42/80], Step [300/500] Loss: 0.3325\n",
            "Epoch [42/80], Step [400/500] Loss: 0.3920\n",
            "Epoch [42/80], Step [500/500] Loss: 0.5988\n",
            "Epoch [43/80], Step [100/500] Loss: 0.3678\n",
            "Epoch [43/80], Step [200/500] Loss: 0.5070\n",
            "Epoch [43/80], Step [300/500] Loss: 0.3517\n",
            "Epoch [43/80], Step [400/500] Loss: 0.5311\n",
            "Epoch [43/80], Step [500/500] Loss: 0.4230\n",
            "Epoch [44/80], Step [100/500] Loss: 0.4121\n",
            "Epoch [44/80], Step [200/500] Loss: 0.4303\n",
            "Epoch [44/80], Step [300/500] Loss: 0.3435\n",
            "Epoch [44/80], Step [400/500] Loss: 0.5321\n",
            "Epoch [44/80], Step [500/500] Loss: 0.3028\n",
            "Epoch [45/80], Step [100/500] Loss: 0.3333\n",
            "Epoch [45/80], Step [200/500] Loss: 0.5007\n",
            "Epoch [45/80], Step [300/500] Loss: 0.5771\n",
            "Epoch [45/80], Step [400/500] Loss: 0.3297\n",
            "Epoch [45/80], Step [500/500] Loss: 0.6156\n",
            "Epoch [46/80], Step [100/500] Loss: 0.3654\n",
            "Epoch [46/80], Step [200/500] Loss: 0.2898\n",
            "Epoch [46/80], Step [300/500] Loss: 0.4028\n",
            "Epoch [46/80], Step [400/500] Loss: 0.3877\n",
            "Epoch [46/80], Step [500/500] Loss: 0.5536\n",
            "Epoch [47/80], Step [100/500] Loss: 0.3128\n",
            "Epoch [47/80], Step [200/500] Loss: 0.5164\n",
            "Epoch [47/80], Step [300/500] Loss: 0.6517\n",
            "Epoch [47/80], Step [400/500] Loss: 0.4390\n",
            "Epoch [47/80], Step [500/500] Loss: 0.3243\n",
            "Epoch [48/80], Step [100/500] Loss: 0.4351\n",
            "Epoch [48/80], Step [200/500] Loss: 0.4253\n",
            "Epoch [48/80], Step [300/500] Loss: 0.4588\n",
            "Epoch [48/80], Step [400/500] Loss: 0.4571\n",
            "Epoch [48/80], Step [500/500] Loss: 0.3720\n",
            "Epoch [49/80], Step [100/500] Loss: 0.5018\n",
            "Epoch [49/80], Step [200/500] Loss: 0.3431\n",
            "Epoch [49/80], Step [300/500] Loss: 0.5142\n",
            "Epoch [49/80], Step [400/500] Loss: 0.3130\n",
            "Epoch [49/80], Step [500/500] Loss: 0.3098\n",
            "Epoch [50/80], Step [100/500] Loss: 0.4945\n",
            "Epoch [50/80], Step [200/500] Loss: 0.3889\n",
            "Epoch [50/80], Step [300/500] Loss: 0.4914\n",
            "Epoch [50/80], Step [400/500] Loss: 0.3935\n",
            "Epoch [50/80], Step [500/500] Loss: 0.5042\n",
            "Epoch [51/80], Step [100/500] Loss: 0.2993\n",
            "Epoch [51/80], Step [200/500] Loss: 0.4657\n",
            "Epoch [51/80], Step [300/500] Loss: 0.2641\n",
            "Epoch [51/80], Step [400/500] Loss: 0.4705\n",
            "Epoch [51/80], Step [500/500] Loss: 0.5318\n",
            "Epoch [52/80], Step [100/500] Loss: 0.2860\n",
            "Epoch [52/80], Step [200/500] Loss: 0.3557\n",
            "Epoch [52/80], Step [300/500] Loss: 0.3546\n",
            "Epoch [52/80], Step [400/500] Loss: 0.4522\n",
            "Epoch [52/80], Step [500/500] Loss: 0.3884\n",
            "Epoch [53/80], Step [100/500] Loss: 0.3781\n",
            "Epoch [53/80], Step [200/500] Loss: 0.4679\n",
            "Epoch [53/80], Step [300/500] Loss: 0.3819\n",
            "Epoch [53/80], Step [400/500] Loss: 0.3800\n",
            "Epoch [53/80], Step [500/500] Loss: 0.4853\n",
            "Epoch [54/80], Step [100/500] Loss: 0.4266\n",
            "Epoch [54/80], Step [200/500] Loss: 0.2853\n",
            "Epoch [54/80], Step [300/500] Loss: 0.3201\n",
            "Epoch [54/80], Step [400/500] Loss: 0.4238\n",
            "Epoch [54/80], Step [500/500] Loss: 0.3110\n",
            "Epoch [55/80], Step [100/500] Loss: 0.5398\n",
            "Epoch [55/80], Step [200/500] Loss: 0.4357\n",
            "Epoch [55/80], Step [300/500] Loss: 0.3722\n",
            "Epoch [55/80], Step [400/500] Loss: 0.5617\n",
            "Epoch [55/80], Step [500/500] Loss: 0.4993\n",
            "Epoch [56/80], Step [100/500] Loss: 0.3152\n",
            "Epoch [56/80], Step [200/500] Loss: 0.2682\n",
            "Epoch [56/80], Step [300/500] Loss: 0.3248\n",
            "Epoch [56/80], Step [400/500] Loss: 0.5066\n",
            "Epoch [56/80], Step [500/500] Loss: 0.3366\n",
            "Epoch [57/80], Step [100/500] Loss: 0.2988\n",
            "Epoch [57/80], Step [200/500] Loss: 0.4138\n",
            "Epoch [57/80], Step [300/500] Loss: 0.4475\n",
            "Epoch [57/80], Step [400/500] Loss: 0.4504\n",
            "Epoch [57/80], Step [500/500] Loss: 0.4926\n",
            "Epoch [58/80], Step [100/500] Loss: 0.5346\n",
            "Epoch [58/80], Step [200/500] Loss: 0.3033\n",
            "Epoch [58/80], Step [300/500] Loss: 0.3166\n",
            "Epoch [58/80], Step [400/500] Loss: 0.6209\n",
            "Epoch [58/80], Step [500/500] Loss: 0.2701\n",
            "Epoch [59/80], Step [100/500] Loss: 0.4016\n",
            "Epoch [59/80], Step [200/500] Loss: 0.2917\n",
            "Epoch [59/80], Step [300/500] Loss: 0.3727\n",
            "Epoch [59/80], Step [400/500] Loss: 0.4499\n",
            "Epoch [59/80], Step [500/500] Loss: 0.3462\n",
            "Epoch [60/80], Step [100/500] Loss: 0.3888\n",
            "Epoch [60/80], Step [200/500] Loss: 0.4918\n",
            "Epoch [60/80], Step [300/500] Loss: 0.4421\n",
            "Epoch [60/80], Step [400/500] Loss: 0.4029\n",
            "Epoch [60/80], Step [500/500] Loss: 0.4399\n",
            "Epoch [61/80], Step [100/500] Loss: 0.3131\n",
            "Epoch [61/80], Step [200/500] Loss: 0.2497\n",
            "Epoch [61/80], Step [300/500] Loss: 0.1759\n",
            "Epoch [61/80], Step [400/500] Loss: 0.4601\n",
            "Epoch [61/80], Step [500/500] Loss: 0.4408\n",
            "Epoch [62/80], Step [100/500] Loss: 0.5080\n",
            "Epoch [62/80], Step [200/500] Loss: 0.4086\n",
            "Epoch [62/80], Step [300/500] Loss: 0.3227\n",
            "Epoch [62/80], Step [400/500] Loss: 0.3765\n",
            "Epoch [62/80], Step [500/500] Loss: 0.3735\n",
            "Epoch [63/80], Step [100/500] Loss: 0.4480\n",
            "Epoch [63/80], Step [200/500] Loss: 0.4654\n",
            "Epoch [63/80], Step [300/500] Loss: 0.5199\n",
            "Epoch [63/80], Step [400/500] Loss: 0.3244\n",
            "Epoch [63/80], Step [500/500] Loss: 0.5460\n",
            "Epoch [64/80], Step [100/500] Loss: 0.4784\n",
            "Epoch [64/80], Step [200/500] Loss: 0.3920\n",
            "Epoch [64/80], Step [300/500] Loss: 0.3145\n",
            "Epoch [64/80], Step [400/500] Loss: 0.4709\n",
            "Epoch [64/80], Step [500/500] Loss: 0.4281\n",
            "Epoch [65/80], Step [100/500] Loss: 0.2884\n",
            "Epoch [65/80], Step [200/500] Loss: 0.5066\n",
            "Epoch [65/80], Step [300/500] Loss: 0.3171\n",
            "Epoch [65/80], Step [400/500] Loss: 0.4145\n",
            "Epoch [65/80], Step [500/500] Loss: 0.5732\n",
            "Epoch [66/80], Step [100/500] Loss: 0.2825\n",
            "Epoch [66/80], Step [200/500] Loss: 0.2589\n",
            "Epoch [66/80], Step [300/500] Loss: 0.3859\n",
            "Epoch [66/80], Step [400/500] Loss: 0.3995\n",
            "Epoch [66/80], Step [500/500] Loss: 0.3215\n",
            "Epoch [67/80], Step [100/500] Loss: 0.3690\n",
            "Epoch [67/80], Step [200/500] Loss: 0.1917\n",
            "Epoch [67/80], Step [300/500] Loss: 0.3003\n",
            "Epoch [67/80], Step [400/500] Loss: 0.3353\n",
            "Epoch [67/80], Step [500/500] Loss: 0.2859\n",
            "Epoch [68/80], Step [100/500] Loss: 0.3796\n",
            "Epoch [68/80], Step [200/500] Loss: 0.3445\n",
            "Epoch [68/80], Step [300/500] Loss: 0.5112\n",
            "Epoch [68/80], Step [400/500] Loss: 0.5263\n",
            "Epoch [68/80], Step [500/500] Loss: 0.5065\n",
            "Epoch [69/80], Step [100/500] Loss: 0.3073\n",
            "Epoch [69/80], Step [200/500] Loss: 0.4118\n",
            "Epoch [69/80], Step [300/500] Loss: 0.2488\n",
            "Epoch [69/80], Step [400/500] Loss: 0.3568\n",
            "Epoch [69/80], Step [500/500] Loss: 0.2758\n",
            "Epoch [70/80], Step [100/500] Loss: 0.4302\n",
            "Epoch [70/80], Step [200/500] Loss: 0.3363\n",
            "Epoch [70/80], Step [300/500] Loss: 0.4364\n",
            "Epoch [70/80], Step [400/500] Loss: 0.3256\n",
            "Epoch [70/80], Step [500/500] Loss: 0.4213\n",
            "Epoch [71/80], Step [100/500] Loss: 0.3171\n",
            "Epoch [71/80], Step [200/500] Loss: 0.4334\n",
            "Epoch [71/80], Step [300/500] Loss: 0.3807\n",
            "Epoch [71/80], Step [400/500] Loss: 0.5289\n",
            "Epoch [71/80], Step [500/500] Loss: 0.4040\n",
            "Epoch [72/80], Step [100/500] Loss: 0.4752\n",
            "Epoch [72/80], Step [200/500] Loss: 0.3632\n",
            "Epoch [72/80], Step [300/500] Loss: 0.3879\n",
            "Epoch [72/80], Step [400/500] Loss: 0.3382\n",
            "Epoch [72/80], Step [500/500] Loss: 0.3560\n",
            "Epoch [73/80], Step [100/500] Loss: 0.2948\n",
            "Epoch [73/80], Step [200/500] Loss: 0.3593\n",
            "Epoch [73/80], Step [300/500] Loss: 0.3985\n",
            "Epoch [73/80], Step [400/500] Loss: 0.4501\n",
            "Epoch [73/80], Step [500/500] Loss: 0.3635\n",
            "Epoch [74/80], Step [100/500] Loss: 0.2581\n",
            "Epoch [74/80], Step [200/500] Loss: 0.3742\n",
            "Epoch [74/80], Step [300/500] Loss: 0.4555\n",
            "Epoch [74/80], Step [400/500] Loss: 0.4267\n",
            "Epoch [74/80], Step [500/500] Loss: 0.4605\n",
            "Epoch [75/80], Step [100/500] Loss: 0.2636\n",
            "Epoch [75/80], Step [200/500] Loss: 0.3095\n",
            "Epoch [75/80], Step [300/500] Loss: 0.3425\n",
            "Epoch [75/80], Step [400/500] Loss: 0.3138\n",
            "Epoch [75/80], Step [500/500] Loss: 0.2806\n",
            "Epoch [76/80], Step [100/500] Loss: 0.3602\n",
            "Epoch [76/80], Step [200/500] Loss: 0.4657\n",
            "Epoch [76/80], Step [300/500] Loss: 0.3486\n",
            "Epoch [76/80], Step [400/500] Loss: 0.3213\n",
            "Epoch [76/80], Step [500/500] Loss: 0.3376\n",
            "Epoch [77/80], Step [100/500] Loss: 0.4578\n",
            "Epoch [77/80], Step [200/500] Loss: 0.2568\n",
            "Epoch [77/80], Step [300/500] Loss: 0.4289\n",
            "Epoch [77/80], Step [400/500] Loss: 0.3784\n",
            "Epoch [77/80], Step [500/500] Loss: 0.3125\n",
            "Epoch [78/80], Step [100/500] Loss: 0.3764\n",
            "Epoch [78/80], Step [200/500] Loss: 0.4958\n",
            "Epoch [78/80], Step [300/500] Loss: 0.3309\n",
            "Epoch [78/80], Step [400/500] Loss: 0.4950\n",
            "Epoch [78/80], Step [500/500] Loss: 0.2387\n",
            "Epoch [79/80], Step [100/500] Loss: 0.3648\n",
            "Epoch [79/80], Step [200/500] Loss: 0.4834\n",
            "Epoch [79/80], Step [300/500] Loss: 0.5522\n",
            "Epoch [79/80], Step [400/500] Loss: 0.3576\n",
            "Epoch [79/80], Step [500/500] Loss: 0.3776\n",
            "Epoch [80/80], Step [100/500] Loss: 0.3603\n",
            "Epoch [80/80], Step [200/500] Loss: 0.3802\n",
            "Epoch [80/80], Step [300/500] Loss: 0.4217\n",
            "Epoch [80/80], Step [400/500] Loss: 0.3122\n",
            "Epoch [80/80], Step [500/500] Loss: 0.2986\n",
            "Accuracy of the model on the test images: 82.77 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}